{"text": "If [imath]X_1, \\dots , X_n[/imath] are iid Poisson([imath]\\lambda[/imath]), find the posterior distribution of [imath]\\lambda[/imath] if the prior on [imath]\\lambda[/imath] is gamma([imath]\\alpha, \\beta[/imath])\n\nLet [imath]X_1,...,X_n[/imath] be iid Poisson([imath]\\lambda[/imath]), and let [imath]\\lambda[/imath] have a gamma([imath]\\alpha, \\beta[/imath]) prior distribution. Find the posterior distribution of [imath]\\lambda[/imath]\nThis is how I first attempted this problem. Since [imath]f(x|\\lambda)=\\dfrac{e^{-\\lambda}\\lambda^x}{x!}[/imath] and [imath]f(\\lambda) = \\dfrac{\\lambda^{\\alpha -1}e^{-\\lambda / \\beta}}{\\Gamma(\\alpha)\\beta^\\alpha}[/imath] then this means that [imath] f(x, \\lambda) = f(x|\\lambda)\\cdot f(\\lambda) = \\dfrac{\\lambda^{x+\\alpha -1}e^{-\\lambda (\\frac{\\beta +1}{\\beta})}}{x!\\Gamma(\\alpha)\\beta^\\alpha } [/imath]. We are asked to find the posterior of [imath]\\lambda[/imath] so we need to find [imath] f(\\lambda | x) = \\dfrac{f(x|\\lambda)f(\\lambda)}{f(x)} [/imath] so we now need to find [imath]f(x)[/imath].\n[imath] f(x) = \\int_0^\\infty f(x,\\lambda)d\\lambda = \\dfrac{\\Gamma(x+\\alpha)(\\frac{\\beta}{\\beta +1})^{x+\\alpha}}{x!\\Gamma(\\alpha)\\beta^\\alpha} \\; \\; x=0,1,2,3,... [/imath]\nTherefore [imath] f(\\lambda |x) = \\dfrac{e^{-\\lambda (\\frac{\\beta+1}{\\beta})}\\lambda^{x+\\alpha -1}}{\\Gamma(x+ \\alpha)(\\frac{\\beta}{\\beta +1})^{x + \\alpha}} [/imath] which means that [imath]\\lambda |x[/imath] has a gamma([imath]x + \\alpha, \\frac{\\beta}{\\beta +1}[/imath]) distribution. \nBut this isnt the correct answer given in the solution manual. The first step in the solution manual is to let [imath]Y=\\sum X_i[/imath] which follows a Poisson([imath]n \\lambda[/imath]) distribution and then do the same exact procedure as I did above except you use [imath]f(y|\\lambda)=\\dfrac{e^{-n\\lambda}(n\\lambda)^x}{x!}[/imath] and [imath]f(\\lambda)[/imath] is still the same. The answer you get by doing it this way is [imath]\\lambda | y[/imath] follows a gamma([imath]x+\\alpha, \\frac{\\beta}{n\\beta +1}[/imath]).\nMy question is why do we first start by letting [imath]Y=\\sum X_i[/imath]? Is this the standard procedure for finding the posterior distribution when given a random sample? \n\nThe likelihood function is for all the data, not just one observation, as in your \"[imath]f(x|\\lambda)[/imath]\".\n@BruceET Right, but what does the likelihood function have to do with this problem?\nBayes' Theorem for this case: \"Posterior = Prior [imath]\\times[/imath] Likelihood\".\nBut [imath]Y= \\sum X_i[/imath] isnt the likelihood function so why are we supposed to use this?\nTwo serious mistakes in all this. First, the joint distribution is [imath]f(x_1,\\ldots,x_n;\\lambda)=f(x_1\\mid\\lambda)\\cdots f(x_n\\mid\\lambda)\\cdot f(\\lambda)[/imath] since there are [imath]n[/imath] observations, not only one. Second it is not (and in this context this is a general fact) necessary to compute [imath]f(x_1,\\ldots,x_n)[/imath] since the posterior density [imath]f(\\lambda\\mid x_1,\\ldots,x_n)[/imath] is, up to some constant independent of [imath]\\lambda[/imath], proportional to [imath]f(x_1,\\ldots,x_n;\\lambda)[/imath]. Thus, forgetting constants independent of [imath]\\lambda[/imath] in each [imath]f(x_k\\mid\\lambda)[/imath], one gets ...\n\nAs already mentioned in the comments, you appear to have some fundamental misconceptions about Bayesian inference, and this is what I intend to address in my response, since you've already received enough computational guidance on your question.\nIf you were given a single observation [imath]X_1 \\mid \\lambda \\sim \\operatorname{Poisson}(\\lambda)[/imath], and you wanted to use a Bayesian approach to make an inference about the posterior distribution of [imath]\\lambda[/imath] based on this observation and your prior beliefs, then this is intuitively and obviously different than if you had many observations in your sample.  Except in certain (unusual) circumstances, the more data you are able to observe from some parametric model, the \"better\" your understanding about what parameter(s) generated that data.\nAfter all, this principle is something you should be familiar with from frequentist statistics:  if you suppose that your observations are generated from a normal distribution with unknown mean [imath]\\mu[/imath] and known standard deviation [imath]\\sigma = 1[/imath], the sample mean is an intuitive and reasonable estimator for [imath]\\mu[/imath]; moreover, the larger your sample, the more likely this estimator will tend toward the true value of [imath]\\mu[/imath].  The same principle applies in Bayesian inference.\nWith this in mind, it should become clear that the likelihood function for [imath]\\lambda[/imath] given the sample, should reflect the size of the sample in some meaningful way.  Your version of the likelihood fails to do that.  Now, you're not obligated to consider the entire sample--effectively, by using only a single observation, you're saying \"I'm going to ignore the rest of the information available to me when calculating the posterior distribution\"--but the posterior you calculate will not be as informative as one that uses all of the data.  Another statistician, seeing your calculation, would not hesitate to improve upon it by using the other observations to produce a superior inference on [imath]\\lambda[/imath].\nClearly, you have some mathematical competence when working with the calculation itself, but there's a deeper conceptual gap that I would invite you to try to close.  Applying formulas is easy.  Understanding what the formulas mean, relating it to the underlying concepts, is the essence of statistical thinking.\n\nI agree that it makes more sense to utilize the sample rather than just one observation like how I originally did. But why are we using [imath]Y = \\sum X_i[/imath] here? My only guess is that because [imath]Y[/imath] is sufficient for [imath]\\lambda[/imath] but I think that may be a coincidence.\n@n.e. That is exactly why the sample total is used.  Sufficiency means that [imath]Y[/imath] preserves all of the information about [imath]\\lambda[/imath] that was present in the original sample.  It is not a coincidence but a property of the Poisson distribution.  The same can be said for some other distributions.  For example, we use this property almost without thinking when we use the Binomial distribution (as a sum of IID Bernoulli variables.\nLet [imath]\\textbf{x} = (x_1, ... , x_n)[/imath] be the observed sample, normally for the bayes estimator we need to use the sample pdf to find the posterior distribution. ie [imath] f(\\theta | \\textbf {x}) = \\dfrac{f(\\textbf{x}|\\theta )f(\\theta)}{f(\\textbf {x})} [/imath]. But if [imath]Y[/imath] is sufficient for [imath]\\theta[/imath] then we can find the same posterior distribution by [imath] f(\\theta | y) = \\dfrac{f(y | \\theta)f(\\theta)}{f(y)} [/imath] ? Thats whats going on in this example. Did in the comments above found the posterior directly using the sample pdf but the solution manual used the sufficient statistic [imath]Y[/imath] to get the same posterior.\nRecall that [imath]Y(\\boldsymbol x)[/imath] is sufficient for [imath]\\theta[/imath] if there exist functions [imath]h[/imath] and [imath]g[/imath] such that [imath]f(\\boldsymbol x \\mid \\theta) = h(\\boldsymbol x) g(Y(\\boldsymbol x) \\mid \\theta).[/imath]  This is the Fisher-Neyman factorization theorem.  It immediately follows that [imath]f(\\theta \\mid \\boldsymbol x) \\propto g(y \\mid \\theta) f(\\theta) \\propto f(\\theta \\mid y).[/imath]  That is why the posterior ends up being the same.\nThat's cool how that works out, thanks for the help!\n\n", "url": "http://math.stackexchange.com/questions/2231052/if-x-1-dots-x-n-are-iid-poisson-lambda-find-the-posterior-distributio"}