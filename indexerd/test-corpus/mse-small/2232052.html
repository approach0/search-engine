<html>
<head>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG" type="text/javascript" charset="utf-8"></script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
	tex2jax: {
		inlineMath: [['[imath]','[/imath]']],
		displayMath: [['[dmath]','[/dmath]']]
	},

	showMathMenu: false, /* do not show menu */
	menuSettings: {CHTMLpreview: false}
});

/* use SVG to avoid chrome trailing space vertical line bug */
MathJax.Hub.Register.StartupHook("End Jax",function () {
	var jax = "SVG";
	return MathJax.Hub.setRenderer(jax);
});
</script>
</head>
<body>
<h2>This is a math-content preview for <a href="http://math.stackexchange.com/questions/2232052/why-my-algorithm-proximal-gradient-descent-algorithm-does-not-converge-when-i-us">http://math.stackexchange.com/questions/2232052/why-my-algorithm-proximal-gradient-descent-algorithm-does-not-converge-when-i-us</a></h2>
Why my algorithm proximal gradient descent algorithm does not converge when I use momentum?</br></br>I'm trying to solve following convex optimization function:</br>[imath]min_W g(W) + h(W)[/imath], where the [imath]g[/imath] is convex and differentiable and [imath]h[/imath] in convex an non-smooth.</br>[imath]g(W)=||Y-WX||_F^2[/imath] is square loss function.</br>Note that,[imath]X,W,Y[/imath] all are matrices.</br>[imath]h(W)[/imath] is non-smooth function with known proximal operator (shoft-tresholding(shirnkage(W))).</br>I use fixed step size (t_k=0.02). I tried to do backtracking line search, but I was not sure how to extend it to the case where [imath]Y and W[/imath] are matrices (when they are both vectors, it's pretty easy to do it)</br>Unfortunately when I use momentum (Nestrove acceleration approach), my algorithm does not converge.</br>Here how I compute momentum:</br>[imath]W^{(0)}=W^{(-1)} \in R^n[/imath], we repeat:</br>[imath]v=W^{(W-1)} + \frac{k-2}{k-1}(W^{(k-1)}-W^{(k-1)})[/imath]</br>[imath]W^{(k)}=prox_{tk}(v-t_k\nabla g(v))[/imath]</br>for k=1,2,3,....</br>Does anybody have some hints regarding computing momentum for accelerared proximal gradient descent method ?</br>second, is it possible to adapt back-tracking line search for the case where [imath]Y and  W[/imath] are matrices ? </br></br>You're probably referring to a work by Nesterov. Please provide all info: which paper are you referring to precisely ? What is [imath]t_k[/imath] provide the exact expression you're using) ?, etc.</br>I would like to add momentum to the proximal gradient descent and run it with fixed step-size.  t_k is step size.</br>What is the value of [imath]t_k[/imath] you're using ? How's it computed ? As your question stands it ap,pears you're trying ad hac amendments to an unknown algorithm. Talking about momentum, what paper are you referring to ? The more details you provide, the more help you'll get here. Nobody will invest more effort answering your question than the effort you invest writing it down properly in the first place...</br>If [imath]t_k = 0.02  > 1 / L_{\nabla g} = 1 / \|X\|_2^2[/imath], then your algorithm might diverge...</br>I gave it to so in my previous comment: [imath]L_{\nabla g} = \|X\|_2^2 = \sigma_{\text{max}}^2(X)[/imath].</br></br>
</body>
</html>
